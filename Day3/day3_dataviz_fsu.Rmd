---
title: "Day 3: Data Visualization in `R`"
author: "Therese Anders"
date: "5/8/2019"
output:
  html_document:
    keep_md: true
  pdf_document:
    number_sections: yes
    toc: yes
subtitle: FSU Summer Methods School
urlcolor: blue
---

# Visualizing regression results
Today, we will be working with the `politicalInformation` dataset from the `pscl` ("Political Science Computational Laboratory") package.
```{r}
library(tidyverse)

# install.packages("pscl")
library(pscl)
head(pscl::politicalInformation)
```

We make a copy the original data set to an object called `dat` and create a new variable that codes an above average knowledge rating for the factor levels `Fairly High` and `Very High`. We also recode a number the `collegeDegree` and `female` factor variables, because the "Yes" and "No" values create issues when extracting names from the model summary object later on).
```{r}
dat <- pscl::politicalInformation %>%
  
  # Note: below we could also use case_when() to code the aboveav variable
  mutate(aboveav = ifelse(y %in% c("Fairly High", "Very High"), 1, 0),
         collegeDegree = case_when(
           collegeDegree == "Yes" ~ 1, 
           T ~ 0
         ),
         female = case_when(
           female == "Yes" ~ 1, 
           T ~ 0
         ))
```

First, lets plot the outcome variable by age. Unfortunately, we cannot see much because of overplotting.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(dat,
       aes(x = age,
           y = aboveav)) +
  geom_point()
```



The graph suffers from overplotting. We can tweak the opacity and size of the points and use jitter to reduce its extent. Below, we also add a smoothing line to show the trend in the graph.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4, echo = F}
ggplot(dat,
       aes(x = age,
           y = aboveav)) +
  geom_point(position = position_jitter(height = 0.1),
             alpha = 0.2,
             size = 0.6) +
  theme_light() +
  geom_smooth() +
  labs(x = "Age",
       y = "Above average political knowledge")
```

Does the effect vary by gender? The plot below shows that female respondents have a lower probability of being classified as having above average political knowledge across all ages.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
names(dat)
ggplot(dat,
       aes(x = age,
           y = aboveav,
           color = factor(female))) +
  geom_point(position = position_jitter(height = 0.1),
             alpha = 0.2,
             size = 0.6) +
  theme_light() +
  geom_smooth() +
  labs(x = "Age",
       y = "Above average political knowledge")
```



Lets run a regression (logit model) of the probability of the interviewers rating a person as above average on political knowledge.
```{r}
mod1 <- glm(aboveav ~ collegeDegree + female + length + age,
           data = dat,
           family = binomial(link = "logit"))
summary(mod1)
```
 
## Coefficient plot
There are a number of packages that offer off-the-shelf solutions to plotting coefficient plots for regression outcomes. In this workshop, we will create a coefficient plot manually. This will allow you to create coefficient plots for models that are not supported by existing packages.
 
Below, we extract properties of interest from the `mod1` object.
```{r}
str(summary(mod1)$coefficients)
dimnames(summary(mod1)$coefficients)

# Note that dimnames() returns a list object, not a vector
df_mod1 <- data.frame(vars = dimnames(summary(mod1)$coefficients)[[1]],
                      coef = summary(mod1)$coefficients[,1],
                      se = summary(mod1)$coefficients[,2]) %>%
  
  # Computing CIs
  mutate(cilo_95 = coef - 1.96*se,
         cihi_95 = coef + 1.96*se,
         cilo_99 = coef - 2.56*se,
         cihi_99 = coef + 2.56*se) 
```

We can graph the coefficient plot using the `geom_point()` aesthetic for the coefficient and the `geom_linerange()` aesthetic for the 95% confidence intervals.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 4}
ggplot(df_mod1,
       aes(x = vars,
           y = coef)) +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95))
```

Let's add a thinner line for the 99% confidence confidence interval.
```{r, warning = F, message = F, fig.height = 2.5, fig.width = 4}
ggplot(df_mod1,
       aes(x = vars,
           y = coef)) +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.5)
```

Finally, we flip the axes and order the coefficients based on their size to clean up the plot. We also add a line at zero to illustrate which coefficients are statistically significantly different from zero. Note that I add the zero line before the `geom_point()` aesthetic so it is in the background.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
ggplot(df_mod1,
       aes(x = reorder(vars, coef),
           y = coef)) +
  geom_hline(yintercept = 0, alpha = 0.8, linetype = "dashed") +
  geom_point() +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.5) +
  coord_flip() +
  theme_light()
```

Suppose, we estimated another model that incorporates a quadratic term for age (since we saw from the earlier plot that age appears to have a curvilinear effect on the probability of being classified as above average on political knowledge). We can add these estimates to the data frame of regression results and plot them on the same coefficient plot to compare the results.

Below, I estimate a new model, `mod2` that includes the squared `age` variable. Note that I add an indicator for the model number `modnum` that we will later use to visually distinguish the results from both models.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
dat <- dat %>%
  mutate(age2 = age^2)
mod2 <- glm(aboveav ~ collegeDegree + female + length + age + age2,
           data = dat,
           family = binomial(link = "logit"))
summary(mod2)

# Extracting the estimates
df_mod2 <- data.frame(vars = dimnames(summary(mod2)$coefficients)[[1]],
                      coef = summary(mod2)$coefficients[,1],
                      se = summary(mod2)$coefficients[,2]) %>%
  
  # Computing CIs
  mutate(cilo_95 = coef - 1.96*se,
         cihi_95 = coef + 1.96*se,
         cilo_99 = coef - 2.56*se,
         cihi_99 = coef + 2.56*se) %>%
  
  mutate(modnum = 2)
```

Below, I add a `modnum` indicator to the `mod1` data frame and create a joint dataframe using the `bind_rows()` function (similar to `rbind`). Note, that there are more efficient ways to run and combine the results of multiple regression models in a single data frame using loops and lists.
```{r, warning = F, message = F, fig.height = 3, fig.width = 3}
df_all <- df_mod1 %>%
  mutate(modnum = 1) %>%
  bind_rows(df_mod2)
```

We pass the `modnum` variable to both, the shape and the color parameter. We use `position = position_dodge()` to separate the lines and points for the two models. `ggplot2` recognizes the `modnum` variable as continuous and therefore does not want to map it to the shape parameter. We can turn `modnum` in a `factor` variable that can be mapped to a shape inside `aes()`. The graph shows that with the exception of the intercept, the model results are not altered much by the inclusion of the quadratic term.
```{r, warning = F, message = F, fig.height = 3, fig.width = 5}
ggplot(df_all,
       aes(x = reorder(vars, coef),
           y = coef,
           color = factor(modnum),
           shape = factor(modnum))) +
  geom_hline(yintercept = 0, alpha = 0.8, linetype = "dashed") +
  geom_point(position = position_dodge(width = 0.3)) +
  geom_linerange(aes(ymin = cilo_95, ymax = cihi_95),
                 size = 1,
                 position = position_dodge(width = 0.3)) +
  geom_linerange(aes(ymin = cilo_99, ymax = cihi_99),
                 size = 0.3,
                 position = position_dodge(width = 0.3)) +
  coord_flip() +
  theme_light()
```


## Predictive probabilities plot
Logit coefficients are themselves not very informative about effect size, in particular with regard to comparing the relative effect size of each coefficient. In addition, the explanatory variables are on different scales, which makes the effect sizes difficult to compare, both in numerical terms as well as in the coefficient plot above. We can illustrate the size of the effects on the predicted probability of being classified as having above average political knowledge by the interviewers, upon plotting the effect over the entire range of explanatory variables and holding all other variables at their mean (or a baseline value for dichotomous variables).
```{r}
# Male, no college degree
scen_male <- expand.grid(collegeDegree = 0,
                         female = 0,
                         length = mean(dat$length, na.rm = T),
                         age = seq(min(dat$age, na.rm = T), max(dat$age, na.rm = T), 1)) %>%
  mutate(age2 = age^2)

# Below, get estimates on link scale, transform to predicted probabilities
# See https://stats.idre.ucla.edu/r/dae/logit-regression/
df_male <- cbind(scen_male, 
                 predict(mod2, newdata = scen_male, type = "link", se = TRUE)) %>%
  mutate(predProb = plogis(fit),
         cilo = plogis(fit - (1.96 * se.fit)),
         cihi = plogis(fit + (1.96 * se.fit))) 
```

Below, we use `geom_ribbon()` to graph the confidence interval around the age estimate. Note that we could use `geom_linerange()` instead.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(df_male, 
       aes(x = age,
           y = predProb)) +
  geom_line() +
  geom_ribbon(aes(ymin = cilo, ymax = cihi),
              alpha = 0.3)
```

**Exercise** Suppose we wanted to compare the effect of age on recorded political knowledge for male and female respondents. 

Please compute the predicted probability of being classified as having above average political knowledge for female respondents, combine the two data frames into one, and graph the results for both male and female respondents in the same plot. Try to re-create the graph below as closely as possible.
```{r, warning = F, message = F, fig.height = 3, fig.width = 5}
scen_female <- expand.grid(collegeDegree = 0,
                         female = 1,
                         length = mean(dat$length, na.rm = T),
                         age = seq(min(dat$age, na.rm = T), max(dat$age, na.rm = T), 1)) %>%
  mutate(age2 = age^2)

df_both <- cbind(scen_female, 
                 predict(mod2, newdata = scen_female, type = "link", se = TRUE)) %>%
  mutate(predProb = plogis(fit),
         cilo = plogis(fit - (1.96 * se.fit)),
         cihi = plogis(fit + (1.96 * se.fit))) %>%
  bind_rows(df_male)

#plotting effect for male and female respondents
ggplot(df_both, 
       aes(x = age,
           y = predProb,
           color = factor(female),
           fill = factor(female))) +
  geom_line() +
  geom_ribbon(aes(ymin = cilo, ymax = cihi),
              alpha = 0.3,
              color = NA) +
  
  # adjusting the appearance of the plot
  scale_color_manual(values = c("0" = "darkblue", 
                                "1" = "darkorange"),
                     name = "",
                     labels = c("Male", "Female")) +
  scale_fill_manual(values = c("0" = "darkblue", 
                                "1" = "darkorange"),
                     name = "",
                     labels = c("Male", "Female")) +
  theme_light() +
  labs(x = "Age",
       y = "Predicted probability of above average",
       title = "Effect of age and gender on political knowledge") +
  coord_cartesian(ylim = c(0,0.6))
```


# Maps in R
## Using `geom_polygon()`
The `R` programming environment offers many powerful tools for the visualization and analysis of spatial data. In this workshop, we focus on the visualization of data using maps. There is a myriad of packages for and approaches to creating maps in `R`. We concentrate on visualizing spatial data with the `ggplot2`  package.

In this first example, we will use map data that is part of the `maps` package in `R` and does not require significant preprocessing before plotting. The `maps` package (https://cran.r-project.org/web/packages/maps/index.html) contains data on lines and polygons for a number of geographical units, including but not limited to, countries of the world, a database of large lakes, as well as United States federal states, counties, and cities.

As a first example, we will create a simple map of the continental United States. We draw the data from the `maps` package and plot it using `ggplot2`.
```{r, warning = F, message = F}
# install.packages("maps")
library(maps)
states_map <- map_data("state")
```

Let us look at the structure of the data we drew from `maps`. The data is stored as a data frame and contains observations that are characterized by unique combinations of longitude and latitude values. Each observation has the following attributes: group, order, region, and subregion if applicable. 
```{r, warning = F, message = F}
head(states_map)
```

The `group` and `order` variables in the data set code relational information of the points. For example, there are 49 regions (all states minus Alaska and Hawaii, plus District of Columbia), 63 groups, and a varying number of points (observations) within each group. The observations denote the border points we plotted previously, and the order counter establishes the sequence in which they should be plotted.
```{r, warning = F, message = F}
head(table(states_map$region))
```

We can use `ggplot2`'s `geom_polygon()` function to plot the observations as polygons, rather than points. In order to do that, we need to specify the grouping parameter. If the data wasn't ordered correctly, we could use the `arrange()` function in the `dplyr` package to establish the correct sequencing of points for each polygon (see demonstration below).
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon()


# Demonstration: messing up the order and creating modern art
states_map_unordered <- states_map %>%
  arrange(long)
ggplot(states_map_unordered, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon()
```

We are operating within the normal `ggplot2` environment, so all regular graphing parameters can be used with maps as well.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkolivegreen", 
               color = "lightgrey", 
               alpha = 0.5) +
  theme_light()
```

The map appears to be a bit "squished". This is because the scaling of the x-axis and y-axis is not based on the scaling of longitude and latitude. We can pass special mapping parameters to `ggplot2` via the `coord_map()` command to achieve the right aspect ratio or use different map projections.
```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkcyan", 
               color = "darkblue", 
               alpha = 0.5) +
  theme_minimal() +
  coord_map()
```

```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(states_map, aes(x = long, 
                       y = lat, 
                       group = group)) +
  geom_polygon(fill = "darkcyan", 
               color = "darkblue", 
               alpha = 0.5) +
  theme_minimal() +
  coord_map("polyconic")
```

If you need world maps that capture the historic borders of countries, take a look at Nils Weidmann, Doreen Kuse, and Kristian S. Gleditsch's `cshapes()` package.


### Plotting points on a map
`ggplot2` allows for plotting in layers. We can use this feature to add points to our map of the continental US. For ease, we will use the data included in the `maps` package. The `us.cities` database contains information on US cities with a population greater than 40,000 and all state capitals. The database contains information on cities in Hawaii and Alaska as well. We will drop these observations before plotting them on our map of the continental US using the `dplyr` package.
```{r, warning = F, message = F, fig.height = 3, fig.width = 4}
cities <- us.cities
table(cities$country.etc)

library(dplyr)
cities_sub <- cities %>%
  filter(!(country.etc %in% c("AK", "HI")))
```


We plot the `cities_sub` data as an additional layer on top of the map of the continental US.
```{r, fig.height = 4, fig.width = 5, warning = F, message = F}
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long,
                   y = lat,
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_light() +
  coord_map() +
  geom_point(data = cities_sub, 
             aes(x = long, 
                 y = lat),
             alpha = 0.8) +
  labs(title = "Cities with over 40,000 inhabitants")
```

Again, we can use `ggplot2`'s regular graphing options in maps. In the following map we will use different colors for state capitals and denote the population of the cities through the size of the points.
```{r, warning = F, message = F, fig.height = 4, fig.width = 5}
table(cities_sub$capital)
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long, 
                   y = lat, 
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_minimal() +
  coord_map() +
  geom_point(data = cities_sub, aes(x = long, 
                                    y = lat, 
                                    color = factor(capital),
                                    size = pop),
             alpha = 0.7) +
  scale_color_manual(values = c("0" = "darkgrey", 
                                "2" = "red"), 
                     labels = c("City over 40,000", 
                                "State Capital"),
                     name = "Type") +
  scale_size_continuous(name = "Population") +
  theme(legend.position = "bottom") +
  labs(title = "Cities with over 40,000 inhabitants")
```

### Adding text
We can also use the names of the observations to label the state capitals.
```{r, warning = F, message = F, fig.height = 4, fig.width = 5}
ggplot() +
  geom_polygon(data = states_map, 
               aes(x = long, 
                   y = lat, 
                   group = group),
               fill = "lightgrey", 
               color = "black", 
               size = 0.2, 
               alpha = 0.2) +
  theme_minimal() +
  coord_map() +
  geom_point(data = cities_sub, aes(x = long, 
                                    y = lat, 
                                    color = factor(capital),
                                    size = pop),
             alpha = 0.6) +
  scale_color_manual(values = c("0" = "darkgrey", 
                                "2" = "red"), 
                     labels = c("City over 40,000", "State Capital"),
                     name = "Type") +
  scale_size_continuous(name = "Population") +
  theme(legend.position = "bottom") +
  geom_text(data = subset(cities_sub, capital == 2),
            aes(x = long, y = lat, label = name),
            size = 2.5) +
  labs(title = "US state capitals and cities with over 40,000 inhabitants",
       x = "",
       y = "")
```

### Subsetting maps
Since the spatial data is stored in a normal database, we can use subsetting to create maps of geographic units contained within a larger spatial database. For example, we could plot a map of Florida using the data from the `maps` package. We can either subset the data before plotting, or use the subset function when specifying the data frame within `ggplot2`.

```{r, fig.height = 3, fig.width = 4, warning = F, message = F}
ggplot(subset(states_map, 
              region == "florida"), 
       aes(x = long, 
           y = lat, 
           group = group)) +
  geom_polygon() +
  coord_map()
```

## `sf` package
`sf` (simple features) is a package for manipulating and analyzing spatial data in `R` (see https://github.com/r-spatial/sf). 

One of the most commonly used sources of shape files is the Global Administrative Areas Database (GADM, http://www.gadm.org) that offers shape files for the administrative boundaries for most countries of the world free of charge. Which administrative boundaries are available varies by country. For example, for the United States, we have shape files at levels 0 (country), 1 (state), and 2 (county). For example, for India, shape files are available for levels 0 (country), 1 (state), 2 (district), and 3 (taluk).

Below, we get data from GADM database for Pakistan via the `raster` package. Data from the GADM by default is stored as a `SpatialPolygonsDataFrame` object. Take a look at the structure of the `SpatialPolygonsDataFrame` below using `View()`.
```{r, fig.height = 4, fig.width = 3}
library(raster)

# Entire country as 1 geom 
pak0 <- getData('GADM', country = 'PAK', level = 0)
# View(pak0)

# Districts
pak3 <- getData('GADM', country = 'PAK', level = 3) 
```

Sometimes plotting maps is a very slow process, especially if many border points are used to plot polygons. Below, we use the `ms_simplify()` function from the `rmapshaper` package to simplify the `SpatialPolygonsDataFrame` (i.e. use fewer points to represent the polygon). We can then transform it to a simple feature (`sf`) object using `st_as_sf()`; using a pipe to connect the operations.
```{r}
library(sf)
library(rmapshaper)

pak3_simple <- ms_simplify(input = pak3, 
                           keep = 0.015) %>%
  st_as_sf()
```

We can plot the `sf` object using `geom_sf()`.
```{r, message = F, fig.height = 4, fig.width = 3}
ggplot() +
  geom_sf(data = pak3_simple)
```


## Adding events to the map
We use the same ACLED data on battles and violence against civilians in Pakistan as in the previous session.  We use the `st_as_sf()` function to turn the `acled` data frame into an `sf` object; specifying `longitude` and `latitude` for the  `coords` parameter. To plot the data, we need to set a coordinate reference system (crs) inside `st_as_sf()`. Here, we use CRS wgs84 (http://download.geonames.org/export/dump/readme.txt). To plot the event locations on the map using `geom_sf()`, the coordinate reference systems of the `geom_sf()` layers have to match.
```{r, warning = F, message = F, fig.height = 4, fig.width = 3}
library(readr) #contains read_csv() function
library(lubridate)
acled <- read_csv("/Users/thereseanders/Documents/UNI/USC/Resources/R/workshop-dataviz-fsu/Day2/1900-01-01-2019-05-03-Pakistan.csv") 
names(acled)

acled_sf <- acled %>%
  
  # using lubidate to re-format the event_date
  mutate(date = dmy(event_date)) %>%
  
  # turning data frame into sf object and setting crs
  st_as_sf(coords = c("longitude", 
                      "latitude"),
           crs = 4326)

# Make sure CRS is the same
st_crs(acled_sf)
st_crs(pak3_simple)
```

We plot the events to the map adding another `geom_sf()` layer. Below, we subset the `acled_sf` dataframe to the year 2015 to reduce the amount of data to plot.
```{r, warning = F, message = F, fig.height = 4, fig.width = 3}
ggplot() +
  geom_sf(data = pak3_simple) +
  geom_sf(data = subset(acled_sf, year == 2015))
```

We can use the `ggplot2` grammar of graphics on spatial data. Below, we plot a separate plot per year and distinguish between battle events and events involving violence against civilians with the `fill` and `shape` aesthetics. Note that the graph takes a moment to render. 
```{r, warning = F, message = F, fig.height = 8, fig.width = 8}
ggplot() +
  geom_sf(data = pak3_simple,
          color = "grey",
          fill = "lightgrey",
          alpha = 0.4) +
  geom_sf(data = subset(acled_sf, year %in% seq(2016, 2018)),
          aes(color = event_type,
              shape = event_type),
          alpha = 0.5) +
  facet_wrap(~ year, nrow = 1) +
  theme_light() +
  coord_sf()
```

## Choropleth maps
Choropleth maps use differences in shading of specific geographic regions to visualize data. 

We can use the `st_join()` function to merge the points and polygons data frame. Here, we implicitly declare `pak3_simple` to be the master data frame, and "add on" `acled_sf`. This preserves the geometry of the polygons and adds the point pattern events data by duplicating the respective polygon information.

We can then compute the number of events per polygon using the `group_by()` and `summarize()` functions.

Note that below, there are a number of polygons that do not experience any events from our ACLED excerpt, which causes the polygon to be dropped. We therefore drop the `geometry` column using `st_set_geometry(NULL)`. We then set up an empty data frame with all possible observations using `expand.grid()` and regular `left_join()` operations (joining a data frame and an `sf` object): a) for the polygon data from `pak3_simple` and b) `df_sum` data frame that contains the summary information.
```{r}
df_sum <- pak3_simple %>%
  st_join(acled_sf) %>%
  group_by(NAME_3, event_type, year) %>%
  summarise(fatal = sum(fatalities, na.rm = T),
            eventcount = n()) %>%
  st_set_geometry(NULL)

df_full <- expand.grid(NAME_3 = unique(pak3_simple$NAME_3),
                       year = unique(acled_sf$year),
                       event_type = c("Battles", "Violence against civilians")) %>%
  left_join(pak3_simple) %>%
  left_join(df_sum)
```

We can now map, for example, the `eventcount` variable to the `fill` aesthetic inside `geom_sf()`. To reduce the number of data plotted, we subset the data to the years 2016, 2017, and 2018, and plot each year in a separate facet.
```{r, warning = F, message = F, fig.height = 4, fig.width = 8}
ggplot() +
  geom_sf(data = subset(df_full, year %in% seq(2016, 2018)),
          aes(fill = eventcount),
          color = "grey") +
  facet_wrap(~ year) +
  scale_fill_gradientn(colors = c("white", "orange", "darkorange", "red", "darkred"),
                       name = "Number of Events",
                       na.value = "white") +
  theme_light()
```

We can further distinguish between the two event types using `facet_grid()`.
```{r, warning = F, message = F, fig.height = 8, fig.width = 8}
ggplot() +
  geom_sf(data = subset(df_full, year %in% seq(2016, 2018)),
          aes(fill = eventcount),
          color = "grey") +
  facet_grid(event_type ~ year) +
  scale_fill_gradientn(colors = c("white", "orange", "darkorange", "red", "darkred"),
                       name = "Number of Events",
                       na.value = "white") +
  theme_light() +
  coord_sf()
```


## Using Stamen map server in `R`
An alternative to shape files is the use of online map data as the basis for the visualization of spatial data. The `ggmap` and `tmaptools` packages allow us to directly query the Google Maps or Stamen Maps servers for a map. Note that due to a recent change in the access policy to Google Maps, you need a google maps API access key to access Google Maps using `ggmap`. In this tutorial, we will use a map from [Stamen](http://maps.stamen.com/#watercolor/12/37.7706/-122.3782).
```{r, message = FALSE, warning = FALSE, fig.height = 4}
# Accessing Stamen map
# https://stackoverflow.com/questions/52704695/is-ggmap-broken-basic-qmap-produces-arguments-imply-differing-number-of-rows/52710855#52710855
library(tmaptools)
library(ggmap)
# default is watercolor
la <- ggmap(get_stamenmap(rbind(as.numeric(paste(geocode_OSM("Los Angeles county")$bbox))), 
                          zoom = 8))
la
```

We can change the appearance of the plot using the `maptype` parameter. To get a list of all types for Stamen maps, see `??get_stamenmap`.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
la_bw <- ggmap(get_stamenmap(rbind(as.numeric(paste(geocode_OSM("Los Angeles county")$bbox))), 
                          zoom = 8,
                          maptype = "toner-lite"))
la_bw
```

### Assessing Los Angeles Air Quality
We can plot additional information on the maps we retrieved from Stamen Maps. As an example, let us consider air quality measurements. As of Spring 2019, the United States Environmental Protection Agency (EPA) still publishes fine-grained geo-coded data on their air quality measurement stations. Here we use a data file that contains 24-hour average fine particulate matter (PM2.5, Federal Reference Method) readings for all US measurement stations in 2018 (Source: file `daily_88101_2018.zip` from https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI). Unzip the file and load the csv using `read_csv()` from the `readr` package below.
```{r}
library(readr)
aqi <- read_csv("daily_88101_2018.csv")
names(aqi)
table(aqi$`Sample Duration`)

aqi_la <- aqi %>%
  filter(`Sample Duration` == "24 HOUR") %>%
  filter(`County Name` == "Los Angeles")
```

Below, we compute the median fine particulate matter air quality index for each LA county site in 2018.
```{r}
table(aqi_la$`Local Site Name`)
aqi_la_summary <- aqi_la %>%
  
  # Grouping by Longitude, Latitude, `Local Site Name` to retain variables
  dplyr::group_by(Longitude, Latitude, `Local Site Name`) %>%
  dplyr::summarise(median_aqi = median(AQI, na.rm = T))
```

We then plot the air quality data onto the map of Los Angeles we retrieved from Stamen maps above, letting the color of the points represent the median air quality index for each station. The air quality gets worse as we move closer to Downtown Los Angeles.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
la_bw +
  geom_point(data = aqi_la_summary, 
             aes(x = Longitude, 
                 y = Latitude, 
                 color = median_aqi), 
             size = 10, 
             alpha = 0.7) +
  scale_color_gradient(low = "green", 
                       high = "red", 
                       name = "Median PM2.5") +
  labs(title = "LA County Air Quality Index for\nFine Particulate Matter in 2018") +
  coord_map(ylim = c(33.5, 34.8))
```

## Density Map of Fine Particulate Matter Pollution
According to the EPA, fine particulate matter PM2.5 should not exceed 35 microcrams per cubic meter of air in a 24-hour average (https://www.epa.gov/criteria-air-pollutants/naaqs-table). Rather than looking at the annual median value of the air quality standard to assess the level of air pollution, we could measure the number of times a station reading exceeds this standard. 

To do this for the continental states, we use a subset of our earlier U.S. states map from the `maps` package. Below, we create an `sf` object from the "state" map.
```{r, message = FALSE, warning = FALSE}
us_map <- st_as_sf(map("state", 
                       plot = FALSE, 
                       fill = TRUE))

ggplot(us_map) +
  geom_sf()
```

We then subset the 2018 EPA data to include only cases where the 24-hour average PM2.5 reading exceeded the 35 microcrams per cubic meter of air standard.
```{r, message = FALSE, warning = FALSE}
pm25 <- aqi %>%
  dplyr::filter(`Sample Duration` == "24 HOUR") %>%
  dplyr::filter(`Arithmetic Mean` >= 35,
                !(`State Name` %in% c("Hawaii", "Alaska")))
```

Finally, we plot the density of observations that exceed the daily air quality standard on the map using `ggplot2`'s `stat_density2d()` function. Note that we are not mapping a specific variable, but rather plot the frequency and clustering of observations in our `pm25` subset of all station readings in 2018. This is achieved through the `stat(level)` argument that is passed to the `fill` parameter inside the `stat_density2d()` function.
```{r, message = FALSE, warning = FALSE, fig.height = 4}
ggplot() +
  geom_sf(data = us_map) +
  geom_point(data = pm25, 
                 aes(x = Longitude, 
                     y = Latitude),
             alpha = 0.5,
             color = "red")


ggplot() +
  geom_sf(data = us_map) +
  stat_density2d(data = pm25, 
                 aes(x = Longitude, 
                     y = Latitude,
                     
                     #mapping observations not any specific variable
                     fill = stat(level)),
                 
                 alpha = 0.4,
                 geom = "polygon") +
  scale_fill_gradient(low = "black", high = "red", 
                       name = "Density of daily\nair quality exceeding\nPM2.5 Standard") +
  labs(title = "Fine Particulate Matter Pollution in 2018")
```
`

